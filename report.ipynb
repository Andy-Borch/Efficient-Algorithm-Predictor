{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97dcdd9f-5f7a-451a-b95c-c2fac44deb99",
   "metadata": {},
   "source": [
    "# CS345 Final Project Report\n",
    "### By: Xander Gupton and Andy Borch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abd5e8-4588-46b5-ab99-7f2d07ac0f5e",
   "metadata": {},
   "source": [
    "For our project, we wanted to investigate how different features of an array effected the efficiency of various sorting algorithms and if that effect could be predicted using machine learning. We sought to create a dataset of arrays and feed them into a variety of popular sorting algorithms to gather data on the efficiency of the algorithms. \n",
    "\n",
    "Initally, we searched for a dataset which would contain the proper data we required, but after some digging we came up empty handed. Thus, we decided to use our newfound knowledge and experience with Numpy to build our own dataset of arrays. This would allow us to control the various features of each array and adjust our data if necessary later on based on more predictive features. The main features we chose to focus on for our arrays were size, mean, standard deviation, and number of unique elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a52173-3a6f-47e7-a4a2-ca3626a223ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m min_unique, max_unique \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     40\u001b[0m num_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m---> 41\u001b[0m data_structure_features \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_data_structure_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#Loop through number of desired arrays and randomize \u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#each feature based on range provided above, then write feature \u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#array to \u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_points):\n",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m, in \u001b[0;36mgenerate_data_structure_array\u001b[1;34m(num_points)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_data_structure_array\u001b[39m(num_points):\n\u001b[1;32m---> 26\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;241m5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(num_points\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m     28\u001b[0m         new_ident \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;241m5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Function which generates a random array given a set of inputs\n",
    "#Size, mean, standard deviation, and number of unique elements all\n",
    "#controllable\n",
    "def generate_array(size, mean=0, std=1, n_unique=10):\n",
    "    if n_unique > size:\n",
    "        raise ValueError(\"Number of unique elements cannot exceed array size.\")\n",
    "\n",
    "    half = n_unique // 2\n",
    "    if n_unique % 2 == 0:\n",
    "        unique_vals = np.arange(-half, half)\n",
    "    else:\n",
    "        unique_vals = np.arange(-half, half + 1)\n",
    "\n",
    "    unique_vals = (unique_vals * std) + mean\n",
    "    unique_vals = unique_vals.astype(np.int64)\n",
    "\n",
    "    sampled = np.random.choice(unique_vals, size=size, replace=True)\n",
    "\n",
    "    return sampled\n",
    "\n",
    "#Code which generates an equally spread matrix of values to \n",
    "#assign data structure type to each array, similar to \"one-hot encoding\"\n",
    "#but instead of creating off of existing data it's randomized with an equal spread.\n",
    "#Creates a set of identity matrices and then shuffles the rows. \n",
    "def generate_data_structure_array(num_points):\n",
    "    features = np.identity(5, dtype=np.int64)\n",
    "    for x in range(int(num_points/5 - 1)):\n",
    "        new_ident = np.identity(5, dtype=np.int64)\n",
    "        features = np.concatenate((features, new_ident))\n",
    "    for y in range(num_points%5):\n",
    "        features = np.vstack((features, [1, 0, 0, 0, 0, 0]))\n",
    "    np.random.shuffle(features)\n",
    "    return features\n",
    "\n",
    "#Ranges for the various variables used in arrays\n",
    "min_size, max_size = 1000, 10000\n",
    "min_mean, max_mean = 10, 100\n",
    "min_stddev, max_stddev = .1, 3\n",
    "min_unique, max_unique = 0.7, 1\n",
    "num_points = 10000\n",
    "data_structure_features = generate_data_structure_array(num_points)\n",
    "\n",
    "#Loop through number of desired arrays and randomize \n",
    "#each feature based on range provided above, then write arrays \n",
    "#and features separate csv files\n",
    "for x in range(num_points):\n",
    "    size = np.random.randint(min_size, max_size)\n",
    "    mean = np.random.randint(min_mean, max_mean)\n",
    "    stddev = np.random.uniform(min_stddev, max_stddev)\n",
    "    percent_unique = np.random.uniform(min_unique, max_unique)\n",
    "    num_unique = int(size * percent_unique)\n",
    "\n",
    "    #write each array to new row in CSV\n",
    "    arr = generate_array(size, mean, stddev, num_unique)\n",
    "    write_to_csv(arr, data_file)\n",
    "\n",
    "    #write features of current array to corresponding line in another csv\n",
    "    features = [size, np.mean(arr), np.std(arr), len(np.unique(arr))]\n",
    "    features = features + data_structure_features[x].tolist()\n",
    "    write_to_csv(features, features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd637bf-26d3-409a-a595-81f41b910811",
   "metadata": {},
   "source": [
    "The above code written by Xander generates two different CSV files, one which contains the raw arrays generated based on the given feature ranges, and one which contains the features of each array as well as a randomly assigned data structure. The different data structures we chose to investigate were python's implementations of arrays, lists, sets, and tuples, as well as Numpy arrays and panda's data structures. \n",
    "\n",
    "Now that we had code to generate our data, we moved onto implementing the various sorting algorithms. Initially, we didn't have a very methodical way of choosing the algorithms we would use. We started by choosing an assortment of algorithms that had a wide range of time complexities, as we felt the model would be able to account for the various differences. We would later learn this was a grave mistake, but that will be explained shortly. The first set of sorting algorithms we chose were merge sort, quick sort, heap sort, bubble sort, insertion sort, bucket sort, and radix sort. One aspect we had to consider was how we would implement the different algorithms for the different data structures we had in our dataset. We decided on casting the various data structures into a type which would work for the algorithm, concluding that the extra time from this casting would be eventually inferred by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92cfd18-2971-4db2-b6f7-d6c17f4a9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort_helper(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort_helper(arr[:mid])\n",
    "    right = merge_sort_helper(arr[mid:])\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left, right):\n",
    "    result = []\n",
    "    i = j = 0\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] < right[j]:\n",
    "            result.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(right[j])\n",
    "            j += 1\n",
    "    result.extend(left[i:])\n",
    "    result.extend(right[j:])\n",
    "    return result\n",
    "\n",
    "def merge_sort(data):\n",
    "    if isinstance(data, (tuple, set, pd.Series)):\n",
    "        data = list(data)\n",
    "    return merge_sort_helper(data)\n",
    "\n",
    "def heapify(arr, n, i):\n",
    "    largest = i\n",
    "    left = 2 * i + 1\n",
    "    right = 2 * i + 2\n",
    "\n",
    "    if left < n and arr[largest] < arr[left]:\n",
    "        largest = left\n",
    "\n",
    "    if right < n and arr[largest] < arr[right]:\n",
    "        largest = right\n",
    "\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]\n",
    "        heapify(arr, n, largest)\n",
    "\n",
    "def heap_sort(data):\n",
    "    if isinstance(data, tuple) or isinstance(data, set):\n",
    "        data = list(data)\n",
    "    n = len(data)\n",
    "    for i in range(n // 2 - 1, -1, -1):\n",
    "        heapify(data, n, i)\n",
    "\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        data[0], data[i] = data[i], data[0]\n",
    "        heapify(data, i, 0)\n",
    "    return data\n",
    "\n",
    "def bubble_sort(data):\n",
    "    if isinstance(data, tuple) or isinstance(data, set):\n",
    "        data = list(data)\n",
    "\n",
    "    n = len(data)\n",
    "    for i in range(n):\n",
    "        for j in range(0, n - i - 1):\n",
    "            if data[j] > data[j + 1]:\n",
    "                data[j], data[j + 1] = data[j + 1], data[j]\n",
    "    return data\n",
    "\n",
    "def insertion_sort(data):\n",
    "    if isinstance(data, tuple) or isinstance(data, set):\n",
    "        data = list(data)\n",
    "    for i in range(1, len(data)):\n",
    "        key = data[i]\n",
    "        j = i - 1\n",
    "        while j >= 0 and key < data[j]:\n",
    "            data[j + 1] = data[j]\n",
    "            j -= 1\n",
    "        data[j + 1] = key\n",
    "    return data\n",
    "    \n",
    "def quick_sort(data):\n",
    "    if isinstance(data, tuple) or isinstance(data, set):\n",
    "        data = list(data)\n",
    "\n",
    "    stack = [(0, len(data) - 1)]\n",
    "\n",
    "    while stack:\n",
    "        start, end = stack.pop()\n",
    "        if start >= end:\n",
    "            continue\n",
    "\n",
    "        pivot = data[end]\n",
    "        i = start\n",
    "        for j in range(start, end):\n",
    "            if data[j] < pivot:\n",
    "                data[i], data[j] = data[j], data[i]\n",
    "                i += 1\n",
    "        data[i], data[end] = data[end], data[i]\n",
    "\n",
    "        stack.append((start, i - 1))\n",
    "        stack.append((i + 1, end))\n",
    "    return data\n",
    "\n",
    "def quick_sort_helper(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = random.choice(arr)\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quick_sort_helper(left) + middle + quick_sort_helper(right)\n",
    "\n",
    "def radix_sort(data):\n",
    "    if isinstance(data, tuple) or isinstance(data, set):\n",
    "        data = list(data)\n",
    "    if len(data) == 0:\n",
    "        return data\n",
    "    \n",
    "    #data = [int(x) for x in data]\n",
    "\n",
    "    max_num = max(data)\n",
    "    exp = 1\n",
    "    while max_num // exp > 0:\n",
    "        counting_sort(data, exp)\n",
    "        exp *= 10\n",
    "    return data\n",
    "\n",
    "def bucket_sort(data):\n",
    "    #Must convert np array and pd Series to list because they\n",
    "    #dont have clear and extend methods\n",
    "    if isinstance(data, (tuple, set, np.ndarray, pd.Series)):\n",
    "        data = list(data)\n",
    "    if len(data) == 0:\n",
    "        return data\n",
    "\n",
    "    min_value = min(data)\n",
    "    max_value = max(data)\n",
    "    bucket_count = len(data)\n",
    "    buckets = [[] for _ in range(bucket_count)]\n",
    "\n",
    "    for num in data:\n",
    "        index = int((num - min_value) * (bucket_count - 1) / (max_value - min_value))\n",
    "        buckets[index].append(num)\n",
    "\n",
    "    data.clear()\n",
    "    for bucket in buckets:\n",
    "        insertion_sort(bucket)\n",
    "        data.extend(bucket)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b61f10-9d8e-4412-aaf9-29527a74f889",
   "metadata": {},
   "source": [
    "### Timing the algorithms\n",
    "With the implementations of each algorithm done by Andy, we were properly able to time how long each array took to run through each algorithm, which we accomplished using the below code created by Andy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d016a7f-cd42-4386-92c7-f40477052408",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m matrix \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m()\n\u001b[0;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m load_features()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matrix)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "matrix = load_data()\n",
    "features = load_features()\n",
    "\n",
    "for i in range(len(matrix)):\n",
    "    arr = np.array(matrix[i])\n",
    "    arr = arr.astype(int)\n",
    "    print(\"Row\", i, \": \")\n",
    "    print(\"Size: \", arr.size, \" : \",features[i][0])\n",
    "    print(\"Mean: \", np.mean(arr), \" : \", features[i][1])\n",
    "    print(\"StdDev: \", np.std(arr), \" : \", features[i][2])\n",
    "    print(\"Num Unique: \", len(np.unique(arr)), \" : \", features[i][3])\n",
    "\n",
    "def time_sort():\n",
    "    timing_data = []\n",
    "    data = convert_to_data_type()\n",
    "\n",
    "    outer_bar = tqdm(range(len(data)), desc=\"Datasets\", unit=\"dataset\", position=0)\n",
    "    #outer_bar = tqdm(range(50), desc=\"Datasets\", unit=\"dataset\", position=0)\n",
    "\n",
    "    for i in outer_bar:\n",
    "        dataset_results = {}\n",
    "        dataset_results[\"Dataset\"] = f\"Dataset {i+1}\"\n",
    "        current_data = data[i]\n",
    "\n",
    "        sorts = [\n",
    "            (\"Bubble\", bubble_sort),\n",
    "            (\"Insertion\", insertion_sort),\n",
    "            (\"Merge\", merge_sort),\n",
    "            (\"Quick\", quick_sort),\n",
    "            (\"Heap\", heap_sort),\n",
    "            (\"Radix\", radix_sort),\n",
    "            (\"Bucket\", bucket_sort),\n",
    "        ]\n",
    "\n",
    "        inner_bar = tqdm(sorts, desc=f\"Sorting Dataset {i+1}\", unit=\"sort\", position=1, leave=False)\n",
    "\n",
    "        for sort_name, sort_fn in inner_bar:\n",
    "            start = time.perf_counter()\n",
    "            sort_fn(current_data)\n",
    "            end = time.perf_counter()\n",
    "            dataset_results[sort_name] = end - start\n",
    "\n",
    "        timing_data.append(dataset_results)\n",
    "\n",
    "    return pd.DataFrame(timing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55c92b-3809-447a-aa08-1c0e184669b9",
   "metadata": {},
   "source": [
    "SHORT EXPLANATION OF TIMING CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8005fd3-8b75-4ccf-a55c-8ee9d34347d9",
   "metadata": {},
   "source": [
    "### Problems with algorithms\n",
    "After we went through and did the first round of timings we ran into an unfortunate problem. There were some of the algorithms which were so much better than others that they outperformed the rest on almost every single array. For example, bucket sort only has a time complexity of O(n + k), while many of the others have complexities of O(n^2) and O(n log n). When we began to create labels for the model to train on, we found that bucket sort was being chosen for every single array, making our data essentially useless as there was no actual prediction to be made. In hindsight, we could have absolutely seen this coming, but, given we didn't we had to find a way to adjust. Thus, we chose to pivot to using only algorithms which had similar time complexities.\n",
    "\n",
    "After doing some research, we found that Heap Sort and Merge Sort had O(n log n) for all cases and Quick Sort had O(n log n) except for in the worst case where it is O(n^2). We made the decision to move forward with only data from these three algorithms, as otherwise we would never be able to train a useful model as the answer could be easily seen from the data with the naked eye. While this would hamper our results moving forward, we felt we could still find useful information about the array's from the more limited scope of the algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18c03f-d5fc-4819-b7d2-4bbca86c64bb",
   "metadata": {},
   "source": [
    "Another roadblock we ran into while timing our algorithms was that Panda Series were extremely slow compared to the other data structures. We initially included them to provide some extra data for the model to train on, but quickly realized they heavily limited the amount of data we could process with the sorting algorithms, thus we decided to remove them from the data entirely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a01ec-994e-455a-85db-16c852edf53b",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "Once we were happy with the state of our data, we moved onto preprocessing our data. We felt there wasn't too much we could perform to improve the model's performance, and thus we decided to mainly focus on standardizing our very large features. Having numbers well into the thousands would only prove to hamper a model's performance, so we standardized each row according to how we learned during the course, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbceef-6f57-4c70-bd0e-e0c26f419a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = load_features()\n",
    "\n",
    "features = np.array(features_list)\n",
    "features = features[:-1, :]\n",
    "features_to_standardize = features[:, :4]\n",
    "\n",
    "floats = features_to_standardize.astype(float)\n",
    "standardized = standardize(floats)\n",
    "\n",
    "print(\"Standard Deviation of each Row should be 1: \", np.std(standardized, axis=0))\n",
    "print(\"Mean of each Row should be 0: \", np.round(np.mean(standardized, axis=0)))\n",
    "\n",
    "features[:, 0:4] = standardized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
